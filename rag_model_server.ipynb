{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_openai langchain_core langchain_community langchain langchain_pinecone jq pypdf bs4 pandas numpy pinecone-client datasets ragas\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"enter key here\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"enter key here\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"enter env here\"\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=\"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\", model=\"gpt-4o\")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant, trained to provide understandable and accurate information about pharmacogenomics and drugs.\n",
    "You will base your responses on the context and information provided. Output both your answer and a score of how confident you are,\n",
    " and also cite the references. Also provide the source of the chunks of the documents used for response.\n",
    "If the information related to the question is not in the context and or in the information provided in the prompt, \n",
    "you will say 'I don't know'.\n",
    "You are not a healthcare provider and you will not provide medical care or make assumptions about treatment.\n",
    "\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pinecone = PineconeVectorStore(embedding=embeddings, index_name=\"pdfs-rag\")\n",
    "\n",
    "retriever = pinecone.as_retriever()\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Questions = pd.read_csv(\"/home/dhanushb/Wellytics/RAG_data/Questions.csv\")\n",
    "\n",
    "questions = Questions[\"Questions\"].to_list()\n",
    "\n",
    "cont = []\n",
    "for question in questions:\n",
    "    cont.append([docs.page_content for docs in retriever.invoke(question)])\n",
    "Questions[\"Context\"] = cont\n",
    "\n",
    "chain = setup | prompt | model | parser\n",
    "resp = []\n",
    "n = len(questions)\n",
    "i = 1\n",
    "for question in questions:\n",
    "    resp.append(chain.invoke(question))\n",
    "    Questions[\"Response\"] = resp + [np.nan] * (n - i)\n",
    "    Questions.to_csv(\"/home/dhanushb/Wellytics/RAG_data/Questions.csv\")\n",
    "    i += 1\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics.critique import harmfulness, strictness, \n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "quests = Questions[Questions[\"Expected_response\"].notna()]\n",
    "\n",
    "for MODEL in MODELS:\n",
    "\n",
    "    # To dict\n",
    "    data = {\n",
    "        \"question\": list(map(str, quests[\"Questions\"].to_list())),\n",
    "        \"answer\": list(map(str, quests[MODEL + \"_resp\"].to_list())),\n",
    "        \"contexts\": [[str(context)] for context in quests[\"context\"].to_list()],\n",
    "        \"ground_truth\": list(map(str, quests[\"Expected_response\"].to_list()))\n",
    "    }\n",
    "\n",
    "    # Convert dict to dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            context_entity_recall,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "            harmfulness\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df = result.to_pandas()\n",
    "\n",
    "    df.to_csv(\"/home/dhanushb/Wellytics/RAG_data/\" + MODEL + \"_eval.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
